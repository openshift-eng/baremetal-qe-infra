#!/bin/bash

function haproxy_cleanup() {
  /usr/local/bin/ovs-docker del-port br-ext eth1 haproxy-"${1}"
  /usr/local/bin/ovs-docker del-port br-int eth2 haproxy-"${1}"
  /usr/local/bin/ovs-docker del-port br-int eth1 haproxy-"${1}"

  docker rm -f haproxy-"${1}" && echo "<4>Deleted haproxy-${1} container"
}

function firewall_cleanup() {
  local SHARED_DIR="/var/builds/${1}"
  bootstrap_host=$(yq eval '.[] | select(.name | test("bootstrap")) | .host' "${SHARED_DIR}/hosts.yaml")
  bootstrap_ip=$(yq eval '.[] | select(.name | test("bootstrap")) | .ip' "${SHARED_DIR}/hosts.yaml")
  if [ -n "${bootstrap_host}" ] && [ -n "${bootstrap_ip}" ]; then
    firewall-cmd --zone=external \
      --remove-forward-port=port=22"${bootstrap_host: -3}":proto=tcp:toport=22:toaddr="${bootstrap_ip}"
    firewall-cmd --zone=external \
      --remove-forward-port=port=22"${bootstrap_host: -3}":proto=tcp:toport=22:toaddr="${bootstrap_ip}" \
      --permanent
  fi

  if [ -n "${bootstrap_ip}" ]; then
    iptables -D FORWARD -s "${bootstrap_ip}" -p tcp --sport 22 -j ACCEPT
    iptables -D FORWARD -s "${bootstrap_ip}" ! -d "${INTERNAL_NET}" -j DROP
    iptables -D FORWARD -s "${bootstrap_ip}" -d "${DOCKER_NET}" -j ACCEPT
  fi

  for nodeip in $(yq eval '.[] | .ip' "${SHARED_DIR}/hosts.yaml"); do
    iptables -D FORWARD -s "${nodeip}" ! -d "${INTERNAL_NET}" -j DROP
    iptables -D FORWARD -s "${nodeip}" -d "${DOCKER_NET}" -j ACCEPT
  done
}

function vlan_cleanup() {
  local SHARED_DIR="/var/builds/${1}"
  if [ -s "${SHARED_DIR}"/vips.yaml ]; then
    ACTION=DELETE
    VLAN_NAME=${1}
    VLAN_ID=$(yq eval '.api_vip' "${SHARED_DIR}"/vips.yaml)
    INTERFACES=$(yq eval '.[] | .switch_port' "${SHARED_DIR}"/hosts.yaml | tr '\n' ',' | sed 's/,$//')
    if [ -n "${INTERFACES}" ]; then
      export ACTION VLAN_NAME VLAN_ID INTERFACES
      python3 /usr/local/bin/set_vlans.py
    fi
  fi
}

function cluster_files_cleanup() {
  local SHARED_DIR="/var/builds/${1}"
  pushd /opt/tftpboot || echo "<3>pushd to /opt/tftpboot failed"
  rm -fr "${1}"
  for mac in $(yq eval '.[] | .mac' "${SHARED_DIR}/hosts.yaml"); do
    grubcfg="grub.cfg-01-$(tr ':' '-' <<< "${mac}")"
    rm -f "${grubcfg}"
  done
  popd || exit 1

  for dir in /home/kni/cluster_configs /var/builds /opt/nfs /opt/html; do
    pushd "${dir}" || continue
    [ -n "${1}" ] && rm -fr "${1}"*
    popd || exit 1
  done

  rm -rf /var/lib/libvirt/openshift-images/*"${1}"*
}

function wait_for_node_status() {
  retry_max=32
  while [ $retry_max -gt 0 ] && ! ipmitool -I lanplus -H "${bmc_address}" -U "${bmc_user}" -P "${bmc_pass}" \
   power status | grep -Eq "${status}"; do
    echo "<4>Waiting for ${name}: ${bmc_address} ${plan}..."
    sleep 15
    retry_max=$(( retry_max - 1 ))
  done
  [ $retry_max -le 0 ] && echo "<3>${bmc_address} needs investigation ${plan}"
}

function reboot_stuck_pdu() {
  local bmc_address="${1}"
  local bmc_user="${2}"
  local bmc_pass="${3}"
  local name="${4}"
  local pdu_uri="${5}"
  local plan="${6}"
  local status="${7}"
  if [ -n "${pdu_uri}" ] && ! ipmitool -I lanplus -H "${bmc_address}" -U "${bmc_user}" -P "${bmc_pass}" \
   power status | grep -Eq ' on| off'; then
    pdu_host=${pdu_uri%%/*}
    pdu_host=${pdu_host##*@}
    pdu_socket=${pdu_uri##*/}
    pdu_creds=${pdu_uri%%@*}
    pdu_user=${pdu_creds%%:*}
    pdu_pass=${pdu_creds##*:}
    echo "${pdu_pass}" > /tmp/ssh-pass
    timeout -s 9 10m sshpass -f /tmp/ssh-pass ssh "${pdu_user}@${pdu_host}" <<EOF
olReboot $pdu_socket
quit
EOF
    pdu_reboot=true
    echo "<3>${name}: ${bmc_address} - pdu rebooted"
    rm -f /tmp/ssh-pass
  else
    echo "<4>${name}: ${bmc_address} - node responded or no pdu for this node"
  fi

  # Wait if a node pdu was rebooted
  if [ "${pdu_reboot}" = "true" ]; then
    echo "<4>Waiting for 2 min before checking node response..." && sleep 120
    wait_for_node_status
  fi
}

function wipe_disk() {
  local bmc_address="${1}"
  local bmc_user="${2}"
  local bmc_pass="${3}"
  local name="${4}"
  local pdu_uri="${5}"
  local plan="${6}"
  local status="${7}"
  echo "<4>Wiping disk of ${name}: ${bmc_address}..."
  ipmitool -I lanplus -H "${bmc_address}" -U "${bmc_user}" -P "${bmc_pass}" \
   chassis bootparam set bootflag force_pxe options=PEF,watchdog,reset,power
  ipmitool -I lanplus -H "${bmc_address}" -U "${bmc_user}" -P "${bmc_pass}" power reset

  # Wait to confirm nodes have powered off
  echo "<4>Waiting for 2 min before checking node power off status..." && sleep 120
  wait_for_node_status
}

function exec_on_nodes() {
  action=${1}
  shift
  params=("${@}")
  for bmhost in $(yq e -o=j -I=0 '.[]' "${SHARED_DIR}/hosts.yaml"); do
    # shellcheck disable=SC1090
    . <(echo "$bmhost" | yq e 'to_entries | .[] | (.key + "=\"" + .value + "\"")')
    if [ ${#bmc_address} -eq 0 ] || [ ${#bmc_user} -eq 0 ] || [ ${#bmc_pass} -eq 0 ] || [ ${#name} -eq 0 ]; then
      echo "Error while unmarshalling hosts entries"
      exit 1
    fi

    # Execute the action
    "${action}" "${bmc_address}" "${bmc_user}" "${bmc_pass}" "${name}" "${pdu_uri:-}" "${params[@]}" &

  done
  wait
}

############
 ## Main ##
############

if [ "$#" -ne 1 ]; then
    echo "<3>Usage: $0 <cluster-name>"
    exit 1
fi

CLUSTER=${1}
INTERNAL_NET=${INTERNAL_NET:-192.168.90.0/24}
DOCKER_NET=${DOCKER_NET:-172.17.0.0/16}
SHARED_DIR="/var/builds/${CLUSTER}"
conf_files=(/opt/dhcpd/root/etc/dnsmasq.conf
            /opt/bind9_zones/zone
            /opt/bind9_zones/internal_zone.rev
            /opt/bind9_zones/external_zone.rev
            /etc/vips_reserved
            /etc/hosts_pool_reserved)

if [ -s "${SHARED_DIR}"/hosts.yaml ]; then
  # Remove firewall rules for bootstrap
  ## Remove port forwarding from bastion host to bootstrap host
  ## Remove bootstrap host access limit to internet if in disconnected network
  ## Remove cluster nodes access limit to internet if in disconnected network
  firewall_cleanup "${CLUSTER}"

  # Delete VLAN on Juniper
  vlan_cleanup "${CLUSTER}"

  # Reboot any stuck node pdu
  exec_on_nodes reboot_stuck_pdu 'for pdu reboot' "' on| off'"

  # Wipe disk and power off
  exec_on_nodes wipe_disk 'to power off' 'Power is off'

else
  echo "<4>${SHARED_DIR}/hosts.yaml file does not exist. Skipping firewall, nodes and vlan cleanup"
fi

# Delete provisioning network
nmcli connection delete "${CLUSTER}"-provisioning-dev
nmcli connection delete br-"${CLUSTER}"-provisioning

# Destroy HAProxy network for $CLUSTER
# Delete haproxy container if it exists for the cluster
haproxy_cleanup "${CLUSTER}"

# Clear cluster entries from the conf files for dhcp, dns, reserved vips if any
# Deallocate reserved nodes for the cluster
sed -i "/${CLUSTER}/d" "${conf_files[@]}"

# Clean up directories used for cluster build
## Clean up HAProxy, cluster, nfs, ignition configuration
## Delete grubcfg files cluster directory in /opt/tftpboot
## Delete cached openshift-images in /var/lib/libvirt/openshift-images/
cluster_files_cleanup "${CLUSTER}"

# Restart dnsmasq container
docker restart dhcpd

# Reload and flush bind9 container
docker exec bind9 rndc reload && \
  docker exec bind9 rndc flush

echo "<4>$CLUSTER - Clean-up, wipe disks and deprovisioning completed successfully"
